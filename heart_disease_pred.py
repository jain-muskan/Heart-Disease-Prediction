# -*- coding: utf-8 -*-
"""Copy of ML-MAJOR-Dinesh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dKz3tgoHXkJTN9OiuN4J35-9-LDLLdjb
"""

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.impute import KNNImputer
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

import warnings
warnings.filterwarnings('ignore')

sns.set(color_codes=True,style='darkgrid')

filename  = 'framingham.csv'
df = pd.read_csv(filename)

#first 5 rows

df.head()

df.shape

df.info()

# STATISTICS

df.describe().transpose()

#checking for missing values

missing_data=df.isnull().sum()
missing_data

missing_vals = missing_data.sum()
tot_vals = np.product(df.shape)
percentage_missing_vals=(missing_vals/tot_vals)*100
print('PERCENTAGE OF MISSING DATA',percentage_missing_vals)

# checking for dupicates

duplicate_df = df[df.duplicated()]
duplicate_df

df.dtypes

# IMPUTING THE MISSING VALUES

knn = KNNImputer(n_neighbors=1)

missing_cols = df[['education','cigsPerDay','BPMeds','totChol','BMI','heartRate','glucose']]

missing = np.array(missing_cols)

imputed_cols = knn.fit_transform(missing)

imputed_cols = pd.DataFrame(imputed_cols,columns=['education','cigsPerDay','BPMeds','totChol','BMI','heartRate','glucose'])

imputed_cols.head()

df = df.drop(['education','cigsPerDay','BPMeds','totChol','BMI','heartRate','glucose'],axis=1)

df.head()

# Concatenating the data 

df = pd.concat([df,imputed_cols],axis=1)

df.head()

df.columns

# Checking again

df.isnull().sum()

df['TenYearCHD'].value_counts()

# Data set is not balanced

# DATA VISUALIZATION

# looking at the distributions of the features

fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax)
plt.show()

sns.distplot(df['age'])

sns.distplot(df['BMI'])

sns.distplot(df['heartRate'])

sns.distplot(df['glucose'])

sns.distplot(df['totChol'])

sns.countplot(df['education'],hue=df['TenYearCHD'])

df.education.value_counts()

sns.countplot(df['currentSmoker'],hue=df['TenYearCHD'])

sns.countplot(df['prevalentHyp'],hue=df['TenYearCHD'])
plt.show()

sns.countplot(df['BPMeds'],hue=df['TenYearCHD'])
plt.show()

sns.countplot(df['diabetes'],hue=df['TenYearCHD'])
plt.show()

sns.countplot(df['male'],hue=df['TenYearCHD'])
plt.show()

sns.boxplot(df['TenYearCHD'],df['totChol'])

plt.figure(figsize=(15,6))
sns.countplot(df['age'],hue=df['TenYearCHD'])
plt.xticks(rotation=40, fontsize=7, ha='right', fontweight='light')
plt.show()

plt.figure(figsize=(15,7))
sns.scatterplot(y=df['diabetes'],x=df['glucose'],hue=df['TenYearCHD'])
plt.show()

sns.countplot(df['male'],hue=df['TenYearCHD'])
plt.show()

sns.pairplot(df)

# INSPECTING THE CORRELATION

corr = df.corr()
plt.figure(figsize=(15,10))
sns.heatmap(corr,annot =True )

# ACCORDING TO DISTRIBUTIONS AND THE HEATMAP(CORRELATION DATA)

# WE CAN REMOVE THE 'EDUCATION' FROM THE DATA BUT LETS KEEP IT AND INSPECT THE ACCURACY

# CHECKING FOR OUTLIERS 

# USING BOXPLOT

sns.boxplot(df.male)

sns.boxplot(df.age)

sns.boxplot(df.sysBP)

sns.boxplot(df.glucose)

sns.boxplot(df.totChol)

sns.boxplot(df.cigsPerDay)

sns.boxplot(df.diaBP)

sns.boxplot(df.diabetes)

sns.boxplot(df.prevalentHyp)

sns.boxplot(df.BPMeds)

# DATA HAS LOTS OF OUTLIERS

# TO HANDLE THE OUTLIERS LETS USE THE ROBUST SCALER WHICH IS GOOD AT HELPING THE MODELS

# FROM GETTING EFFECTED BY THE OUTLIER DATA

X = df.drop(['TenYearCHD'],axis=1)
y = df['TenYearCHD']

# DATA IS NOT BALANCED SO WE USE OVER SAMPLING FROM IMBLEARN 

#to increase the instances of heart disease affected people

# SAMPLING AND

# NORMALIZATION

os = RandomOverSampler(0.5)

#0.5 ==> 50%

x_os,y_os = os.fit_sample(X,y)

x_os.shape

y_os.shape

type(y_os)

x_cols = X.columns

x_os = pd.DataFrame(x_os,columns=x_cols)

y_os = pd.Series(y_os,name='TenYearCHD')

y_os.shape

x_os.shape

# SPLITTING THE DATA

# TRAINING DATA || VALIDATION DATA

x_train,x_test,y_train,y_test = train_test_split(x_os,y_os,test_size=0.4,random_state=0)

x_train.shape

x_test.shape

y_test.value_counts()

# SCALING THE FEATURES

scaler = RobustScaler()

xs_train = scaler.fit_transform(x_train)
xs_test = scaler.fit_transform(x_test)

# IMPLEMENTING MACHINE LEARNING MODELS

# VALIDATING EACH MODEL

# ACCURACY SCORE, PRECISION SCORE, RECALL SCORE, F1 SCORE

"""## Logistic Regression Model"""

logr = LogisticRegression(class_weight='balanced',random_state=0)

model1 = logr.fit(xs_train,y_train)

y_pred1 = model1.predict(xs_test)

xs_test.shape

accuracy_score(y_test,y_pred1)

precision_score(y_test,y_pred1)

recall_score(y_test,y_pred1)

f1_score(y_test,y_pred1)

# plotting the CONFUSION MATRIX 

# Logistic Regression

cnf_matrix_logr = confusion_matrix(y_test, y_pred1)

ax= plt.subplot()
sns.heatmap(pd.DataFrame(cnf_matrix_logr), annot=True,cmap="Reds" , fmt='g')

ax.set_xlabel('Predicted ');ax.set_ylabel('True');

"""## K-neighbors Classifier model"""

knc = KNeighborsClassifier()

from sklearn.model_selection import GridSearchCV

param = {'n_neighbors':[3,5,7,9,11],'weights':['uniform','distance']}

knc_clf = GridSearchCV(knc,param_grid=param,scoring='recall',cv=5,n_jobs=-1,verbose=3).fit(xs_train,y_train)

knc_clf.best_params_

model2 = KNeighborsClassifier(n_neighbors=7,weights='distance',).fit(xs_train,y_train)

y_pred2 = model2.predict(xs_test)

accuracy_score(y_test,y_pred2)

precision_score(y_test,y_pred2)

recall_score(y_test,y_pred2)

f1_score(y_test,y_pred2)

# plotting the CONFUSION MATRIX 

# K-neighbors Classifier

cnf_matrix_knc = confusion_matrix(y_test, y_pred2)

ax= plt.subplot()
sns.heatmap(pd.DataFrame(cnf_matrix_knc), annot=True,cmap="Reds" , fmt='g')

ax.set_xlabel('Predicted ');ax.set_ylabel('True');

"""## Support Vector Machines Model"""

svc = LinearSVC()

model3 = svc.fit(xs_train,y_train)

y_pred3 = model3.predict(xs_test)

accuracy_score(y_test,y_pred3)

precision_score(y_test,y_pred3)

recall_score(y_test,y_pred3)

f1_score(y_test,y_pred3)

svm = SVC(C=750,probability=True,class_weight='balanced')

model_3 = svm.fit(xs_train,y_train)

y3_pred = model_3.predict(xs_test)

accuracy_score(y_test,y3_pred)

precision_score(y_test,y3_pred)

recall_score(y_test,y3_pred)

f1_score(y_test,y3_pred)

# SVC Performs better in this case when compared to Linear SVC

# plotting the CONFUSION MATRIX 

# Support Vector Machines

cnf_matrix_svc = confusion_matrix(y_test, y3_pred)

ax= plt.subplot()
sns.heatmap(pd.DataFrame(cnf_matrix_svc), annot=True,cmap="Reds" , fmt='g')

ax.set_xlabel('Predicted ');ax.set_ylabel('True');

"""## Decision Tree Classifier model"""

dtree = DecisionTreeClassifier(criterion='gini',class_weight='balanced')

model4 = dtree.fit(x_train,y_train)

y_pred4 = model4.predict(x_test)

accuracy_score(y_test,y_pred4)

precision_score(y_test,y_pred4)

recall_score(y_test,y_pred4)

f1_score(y_test,y_pred4)

# plotting the CONFUSION MATRIX 

# Decision Tree Classifier

cnf_matrix_dtree = confusion_matrix(y_test, y_pred4)

ax= plt.subplot()
sns.heatmap(pd.DataFrame(cnf_matrix_dtree), annot=True,cmap="Reds" , fmt='g')

ax.set_xlabel('Predicted ');ax.set_ylabel('True');

"""## XGBoost Classifier"""

model5 = XGBClassifier(learning_rate=0.3,colsample_bynode=0.8,subsample=0.8).fit(x_train,y_train)

y_pred5 = model5.predict(x_test)

accuracy_score(y_test,y_pred5)

precision_score(y_test,y_pred5)

recall_score(y_test,y_pred5)

f1_score(y_test,y_pred5)

# plotting the CONFUSION MATRIX 

# XGBoost Classifier

cnf_matrix_xgb = confusion_matrix(y_test, y_pred5)

ax= plt.subplot()
sns.heatmap(pd.DataFrame(cnf_matrix_xgb), annot=True,cmap="Reds" , fmt='g')

ax.set_xlabel('Predicted ');ax.set_ylabel('True');

"""## LIGHTGBM Classifier"""

lgb_clf = LGBMClassifier(class_weight='balanced',n_jobs=-1,n_estimators=300,subsample=0.6,colsample_bytree=0.5)

model6 = lgb_clf.fit(x_train,y_train)

y_pred6 = model6.predict(x_test)

accuracy_score(y_test,y_pred6)

precision_score(y_test,y_pred6)

recall_score(y_test,y_pred6)

f1_score(y_test,y_pred6)

# plotting the CONFUSION MATRIX 

# LIGHTGBM Classifier

cnf_matrix_lgb_clf = confusion_matrix(y_test, y_pred6)

ax= plt.subplot()
sns.heatmap(pd.DataFrame(cnf_matrix_lgb_clf), annot=True,cmap="Reds" , fmt='g')

ax.set_xlabel('Predicted ');ax.set_ylabel('True');

"""## RandomForest Classifier model"""

rfcl = RandomForestClassifier(class_weight='balanced',n_estimators=250,max_features='auto')

model8 = rfcl.fit(x_train,y_train)

y_pred8 = model8.predict(x_test)

accuracy_score(y_test,y_pred8)

precision_score(y_test,y_pred8)

recall_score(y_test,y_pred8)

f1_score(y_test,y_pred8)

# plotting the CONFUSION MATRIX 

# RandomForest Classifier

cnf_matrix_lgb_clf = confusion_matrix(y_test, y_pred8)

ax= plt.subplot()
sns.heatmap(pd.DataFrame(cnf_matrix_lgb_clf), annot=True,cmap="Reds" , fmt='g')

ax.set_xlabel('Predicted ');ax.set_ylabel('True');

"""## Gradient Bossting Classifier"""

gbcl = GradientBoostingClassifier(n_estimators=1000,learning_rate=0.3,subsample=0.5,max_features=1.0)

model9 = gbcl.fit(x_train,y_train)

y_pred9 = model9.predict(x_test)

accuracy_score(y_test,y_pred9)

precision_score(y_test,y_pred9)

recall_score(y_test,y_pred9)

f1_score(y_test,y_pred9)

# plotting the CONFUSION MATRIX 

# Gradient Bossting Classifier

cnf_matrix_gbcl = confusion_matrix(y_test, y_pred9)

ax= plt.subplot()
sns.heatmap(pd.DataFrame(cnf_matrix_gbcl), annot=True,cmap="Reds" , fmt='g')

ax.set_xlabel('Predicted ');ax.set_ylabel('True');

# We can select RANDOMFOREST CLASSIFIER Model or LIGHTGBM model AND XGBOOST Model for our problem.

# It provides higher accuracy through cross validation.

# If there are more trees, it won’t allow over-fitting trees in the model.

# Overall, it's a better choice for our problem statement.

# With incresing in more training instances we can improve the models performence as we go further.

# Finally these three models are the best perfoming ones for this classification scenario.